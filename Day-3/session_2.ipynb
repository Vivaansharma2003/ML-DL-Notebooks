{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c249aa",
   "metadata": {},
   "source": [
    "# Day 3: Ensemble Learning & Random Forests\n",
    "\n",
    "1.  **Ensemble Learning (Bagging):** Reducing variance using the Wine Dataset.\n",
    "2.  **Random Forests:** Demonstrating robustness against noise and high dimensionality using the Digits Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7911bb7",
   "metadata": {},
   "source": [
    "---\n",
    "## Topic 1: Ensemble Learning (Bagging)\n",
    "**Goal:** Understand how Bootstrap Aggregating (Bagging) reduces the variance of a single model.\n",
    "\n",
    "We will use the **Wine Dataset**. It is small, simple, and prone to overfitting if we use a complex Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912afbf",
   "metadata": {},
   "source": [
    "### Step 1: Install and Import Basics\n",
    "First, we ensure we have the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d1c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e739f050",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset (Wine)\n",
    "We use the Wine dataset. It classifies wines into 3 categories based on 13 chemical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f1315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_wine()\n",
    "\n",
    "# specific features (X) and target (y)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Let's look at the shape of our data\n",
    "print(f\"Features: {X.shape}\")\n",
    "print(f\"Target Labels: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ab19f5",
   "metadata": {},
   "source": [
    "### Step 3: Split the Data\n",
    "We must split the data to see how the model performs on *unseen* data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de91c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split: 70% for training, 30% for testing\n",
    "# random_state ensures we get the same split every time we run this\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Data split complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78e7757",
   "metadata": {},
   "source": [
    "### Step 4: The High Variance Model (Single Decision Tree)\n",
    "We start with a single Decision Tree. Trees are high-variance models; they can change drastically with small changes in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad553be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize a standard Decision Tree\n",
    "tree = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
    "\n",
    "# Train the tree\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "\n",
    "# Calculate Accuracy\n",
    "acc_tree = accuracy_score(y_test, y_pred_tree)\n",
    "print(f\"Single Decision Tree Accuracy: {acc_tree:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ef793a",
   "metadata": {},
   "source": [
    "### Step 5: Bagging (Bootstrap Aggregating)\n",
    "Now, we use Bagging.\n",
    "1. We create multiple subsets of data (Bootstrapping).\n",
    "2. We train a tree on each subset.\n",
    "3. We average the results (Aggregating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f4fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Initialize Bagging Classifier\n",
    "# n_estimators=50 means we create 50 different trees\n",
    "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the ensemble\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_bagging = bagging.predict(X_test)\n",
    "\n",
    "# Calculate Accuracy\n",
    "acc_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "\n",
    "print(f\"Bagging Ensemble Accuracy:    {acc_bagging:.4f}\")\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Improvement: {(acc_bagging - acc_tree)*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fd5c91",
   "metadata": {},
   "source": [
    "**Conclusion for Topic 1:** By averaging 50 trees, the Bagging model smoothed out the errors and performed better on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61256bb",
   "metadata": {},
   "source": [
    "---\n",
    "## Topic 2: Random Forests\n",
    "**Goal:** Demonstrate robustness against high dimensionality and noise.\n",
    "\n",
    "We will use the **Digits Dataset** (images of handwritten numbers). This has 64 features (8x8 pixels), which is higher dimensionality than the previous example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854588b5",
   "metadata": {},
   "source": [
    "### Step 1: Import Visualization Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05dbcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb56344",
   "metadata": {},
   "source": [
    "### Step 2: Load Data (High Dimensionality)\n",
    "We load the Digits dataset. Each row is an image flattened into 64 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "print(f\"Feature shape: {X_digits.shape} (64 dimensions)\")\n",
    "\n",
    "# Let's visualize one sample to understand the data\n",
    "plt.gray()\n",
    "plt.matshow(digits.images[0])\n",
    "plt.title(f\"Target: {y_digits[0]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92902a2e",
   "metadata": {},
   "source": [
    "### Step 3: Add Noise (Robustness Test)\n",
    "To prove Random Forest is \"robust against noise,\" we will intentionally make the problem harder by adding random noise to the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1191fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random noise\n",
    "noise = np.random.normal(0, 4, X_digits.shape)\n",
    "\n",
    "# Add noise to our original data\n",
    "X_noisy = X_digits + noise\n",
    "\n",
    "print(\"Noise added to data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa39dba",
   "metadata": {},
   "source": [
    "### Step 4: Split the Noisy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1a14e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_n, X_test_n, y_train_n, y_test_n = train_test_split(X_noisy, y_digits, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43769ad",
   "metadata": {},
   "source": [
    "### Step 5: Train Random Forest\n",
    "A Random Forest improves on Bagging by also selecting a **random subset of features** at each split. This handles high dimensionality very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c910a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Random Forest\n",
    "# n_estimators=100 (100 trees)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train\n",
    "rf_model.fit(X_train_n, y_train_n)\n",
    "\n",
    "# Predict\n",
    "y_pred_rf = rf_model.predict(X_test_n)\n",
    "\n",
    "# Evaluate\n",
    "acc_rf = accuracy_score(y_test_n, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy (on noisy data): {acc_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b058e60",
   "metadata": {},
   "source": [
    "### Step 6: Why is it robust? (Feature Importance)\n",
    "Random Forests can figure out which pixels (features) actually matter and ignore the noisy ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf93a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Reshape importance array back to 8x8 image to visualize\n",
    "importance_image = importances.reshape(8, 8)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(importance_image, cmap='hot')\n",
    "plt.colorbar(label='Importance')\n",
    "plt.title(\"Which pixels matter most?\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
